\chapter{Towards learn-based level generation}
\paragraph{Overview} The scope of this chapter is to further describe the setting of our work and highlight the main differences between the work that have been currently done in the literature and the one we introduce in the following chapters. In particular, section~\ref{sec:gantheory} first provides the theoretical background necessary to understand the setting in which our work take place. Then, section~\ref{sec:doominfo} describes and motivates the choice of the game we work with. Lastly, section~\ref{sec:considerations} reveals some important considerations about the setting just described and the possible difficulties in this type of research.
%TODO: Bisogna anche sottolineare la congiunzione tra ML e PGC, che ha originato questa tesi
\section{Generative Adversarial Networks}
\label{sec:gantheory}
\subsection{Overview}

\label{sec:introgan}
\paragraph{GAN} A model that in the latest years gained increasingly more interest in the field of Machine Learning research is the one of the Generative Adversarial Network, proposed by \citeauthor{gan} in \citetitle{gan} (\citeyear{gan}) \cite{gan}. The main idea of this kind of generative model is to use two neural networks which are posed in an adversarial setting that models a two-player Minimax Game \cite[p.~276]{minimax}. In particular, a generative network called G is trained to capture the data distribution while a discriminator network D estimates the probability that each sample comes from the real data distribution rather than the one generated by G.
\paragraph{Conditional GAN} In our work we make use of a modification of the GAN model introduced by \citeauthor{conditionalgan} \cite{conditionalgan}, which allows to condition the data generation to class labels. This structure is summarized in section \ref{sec:modelstructure} where the logical structure of the generative model we adopt is presented.

\paragraph{Additional Architectures} Due to the amount of interest that \glspl{gan} have raised, many researches have been conducted to improve the quality performances of \glspl{gan}, leading to a variety of proposed new architectures. They can be classified as modifications to the underlying neural network architectures or even to the conceptual setting of the model itself, by means of changes to the loss function or the training algorithm. Other, less formal but still effective adjustment to the proposed models are the so called "tricks" in machine learning discussion communities, which adoption is often suggested in order to improve the difficult training of the network. The final architecture adopted is described in section \ref{sec:networkarch}.


\subsection{Deep Convolutional GAN}
\subsection{Wesserstein GAN}
\subsection{Wesserstein GAN with Gradient Penalty}
\section{Game of choice: DOOM}
\label{sec:doominfo}
\subsection{Description}
\subsection{Motivation}
% Parla del game engine, dei livelli piatti (limitazione/vantaggio), dataset disponibile, grande community ancora attiva, ben documentato


\section{General Considerations}
\label{sec:considerations}
%TODO: Parla dei diversi setting in cui vengono utilizzate le gan e delle possibili difficolt√†. Sample evaluation problem.
\section{Summary}
