\chapter{Results Evaluation and Conclusions}
\section{Results Evaluation}





\section{General Considerations}
\label{sec:considerations}
%TODO: Parla dei diversi setting in cui vengono utilizzate le gan e delle possibili difficolt√†. Sample evaluation problem.
\paragraph{} Although the model we designed is far for being perfect in solving the level generation problem for 2d non-linear environment such DOOM maps, we think it's still a good starting point for future improvements and could represent a viable alternative to classical Procedural Generation. In particular, most of the levels generated from the network have proved to be interesting to explore and play due to the presence of particular features typical of doom maps, such as narrow tunnels and large rooms. This suggest that one of the advantages of our method with respect to Procedural Generation is that in our case there is no need of an expert designer to embed it's knowledge in the generation process; still, this method allows the designer to focus on the selection of more high-level features as those we selected as network inputs. 

\paragraph{} When using this type of models, one of the most common concerns is that the network could overfit the training set, learning to reproduce the samples of the dataset. While we cannot prove it formally, we refer to the results of \citeauthor{empiricalevaluation} in \cite[Appendix~C]{empiricalevaluation}, which claim that overfitting the model we used in our work is difficult even for a small number of training samples, de facto demonstrating that the behaviour of GANs is different from that of classical deep neural networks used for classification. One result that intuitively supports this consideration is given by the fact that when we generated 1000 levels for each input feature we often obtained feature distributions with large variance, which makes improbable that the discriminator could have overfit the dataset levels.



\section{Future Work}
\paragraph{} In this section we first present the general open problems we encountered in our setting, then we propose some specific works that can be made to improve our system.
\subsection{Open Problems}
\subsubsection{Data Availability}
\paragraph{} The amount of data available is a problem that affects in general every deep learning setting. As discussed in earlier sections, the context of video-games is one of field in which data is less available and uniform. In our work we used a dataset of 1088 levels which have been augmented by rotation due to memory size constraints, but we envision that a larger amount of levels could make the network more accurate in generating levels. We propose a possible improvement for our system in section \ref{sec:data-augmentation}.
\subsubsection{Samples Evaluation}
\paragraph{} As we explained in section~\ref{sec:evaluation}, the problem of evaluating the samples generated by a GAN is still a recent field of research and still a general prevailing model have to be proposed. Moreover, the particular domain of our work makes even more difficult to apply commonly used methods to assess the quality. In section~\ref{sec:evaluation} we proposed a qualitative method for assessing the generated sample quality during the training process according to our data. While the method we applied succeeds in indicating that the network is actually learning the level structures, the metrics we proposed have the drawback that they need to be considered altogether but calculated on each map differently in order to benefit of their informational power.

\subsubsection{Loss of accuracy}
% in particolare sui nemici
\subsubsection{Presence of Noise}

	
\label{sec:sampling}
% Alcune considerazioni su come viene campionata la rete 
% Eventuale sezione sull'interpolazione di livelli



\subsection{Possible Applications and future develops}
\subsubsection{Augmenting input data}
% Augmenting the dataset splitting levels by floor
\label{sec:data-augmentation}
\subsubsection{Better map conversion}
% Better map to wad conversion: Enemies, heights, textures.
\subsubsection{Sample interpolation}
% Sample interpolation
\subsubsection{Tweaking the model}
% Alternative models
\subsubsection{Different architectures}
% Hybrid method based on autoencoders


