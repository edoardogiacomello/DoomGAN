\chapter{Results Evaluation and Conclusions}
\label{ch:results-conclusion}
\paragraph{} In this chapter we give a detailed interpretation of the results obtained by the training of the network and the experiments we conducted. Section \ref{sec:conclusions} reports general consideration about our work, while section \ref{sec:futurework} highlights the possible future develops for enhancing our results.

\section{Results Evaluation}

\paragraph{} We conduct the analysis of the results in the same order as they are presented in the previous chapter, eventually adding some references to content which is included in Appendix for readability. The first part of the discussion considers the metrics that have been monitored during the training phase, explaining the meaning of the results. After that, we consider the results of the experiments described in section \ref{sec:experiments}.
\subsection{Sample Evaluation Metrics}
\paragraph{} Training losses in figures \ref{fig:train-cond-loss} and \ref{fig:train-uncond-loss} confirm the advantages of the WGAN model over earlier proposals, it is indeed possible to appreciate a converging behaviour similar to that of classical Neural Networks in both the conditional and unconditional case. This proved to be useful for understanding when to stop the training phase, since a manual inspection of the samples wouldn't be informative of the actual network capabilities. The fact that the validation loss follows the behaviour of the training loss also suggests a good generalization capability of the critic in assigning level scores even for previously unseen samples. 

\paragraph{} Entropy Mean Absolute Errors in figure \ref{fig:train-entropy-mean} converge toward a small value in the same way the loss does, confirming that the WGAN loss is actually correlated to the sample quality in our case. In particular this metric is related to the difference in information content, or noise, between true and generated samples. While a small difference is still detectable, both networks showed to reduce this difference as the training proceeded.

\paragraph{} Mean Structural Similarity (Figure~\ref{fig:train-similarity}) shows that the conditional network generates samples that are structurally more similar to true levels than the unconditional network does. This is also true if we consider the networks in their early training stage. It's worth nothing that the structural similarity is a metric for evaluating the perceived sample quality, so it reaches unitary values only if the two samples are the same. In our setting we compare different levels so we cannot treat SSIM as an absolute value but only as an indicator of the "relative quality" of the samples generated by the two networks.

\paragraph{} Encoding Error in figures \ref{fig:train-encoding_error-floormap} to \ref{fig:train-encoding-error-thingsmap} shows that the network is capable to easily learn and reproduce the colour coding used in each separated map. This does not mean that the network is able, for example, to only output either the value 0 or 255 when generating a floormap, but this metric shows that the average error is small. For this reason, the only post-processing we apply to generated samples in order to compute the generated features is to threshold the colour value toward the closest meaningful value (eg. a 244 in a floormap is interpreted as a 255). 

\paragraph{} Corner Error shows that while the unconditional network generates levels that never improve their corner count with respect to the true ones, the conditional network actually learns to generate slightly more accurate levels. This value could hardly reach low values as two levels having the same features (with reference to the ones we have selected) could naturally differ in their topology, raising the value of this metric. Another aspect that is worth nothing is the lower variance of the conditional case, possibly reflecting a lower amount of noises or artefacts in generated levels.

\subsection{Experiments Discussion}
\paragraph{} The first column of the tables \ref{tab:results-input-features} and \ref{tab:results-other-features} show for what features it is not possible to reject the hypothesis that the true distribution and the distribution generated by the unconditional network are equal. If we analyse these features we can notice that many features that are "well learned" by the network in their output depend on the level area or the perimeter. In particular, figure \ref{fig:results-input-features} and \ref{fig:results-input-distr-features} show how the two networks are able to reproduce the level area distribution (level\_equivalent\_diameter) and the length of the minor axis of the levels. 

\paragraph{} The second column of the tables shows the same concept for the conditional network. In particular, table \ref{tab:results-input-features} shows substantial improvements on the input features: In this case, the feature  \textit{distmap-kurt} (associated to the "variety" in room dimensions), is now distributed enough closely to the real distribution that the null hypothesis cannot be rejected any more. The input features that have been learned by the unconditional network are also learned by the conditional network. For the features \textit{solidity, nodes (number of rooms)} and \textit{distmap skewness (balance between big and small areas)} the conditional network cannot reproduce a distribution that is close enough to the real one to change the outcome of the test, however the statistic values indicate that the distributions of the conditional network are the closest to the real one.

\paragraph{} If we analyse the results in table \ref{tab:results-other-features} and graphs in Appendix~\ref{sec:appendix-graphs} we can confirm in many case the behaviour stated in the previous paragraph: the features which are better represented by both the network (group F3) are related with the area of the level. Features expressing  locations of particular points of the maps such as the topological centroid or even explicitly represented as the player starting point, are unsurprisingly not well represented by the networks. The features regarding the number of items in the level, which would be useful from a design point of view, are still not well represented probably due to the still too high presence of checker board artefacts in the \textit{Thingsmap}. However, the conditional proved again to be the best of the two networks in describing this set of features, learning particularly well with the distribution of "power-ups". The last set of listed features is, as expected, not well learned by the networks since it is composed by features which are graph-metrics or other too complex features, for which this kind of model is not suitable.

\paragraph{} Graphs of figure~\ref{fig:results-exp3-features} show how the unconditional network response varies by changing each input features to the values that correspond to the three quartiles of the true distribution. Even if in any case the input feature can deterministically control the output feature of the value, for the majority of features it is possible to alter the output distribution by acting on the input value. This fact is represented by the ordering of the curves, in many case reflecting the ordering of the requested values.
In particular we can notice how all the features except the \textit{solidity} and \textit{distmap-kurt} react well to changes to values. The particular shape of some curves such in the \textit{distmap-kurt} and \textit{major axis} may suggest that the translation of the output features does not follow linearly the translation of the input feature or it could work only in some areas of the input feature space. The particular undesired behaviour of the 75th percentile of \textit{level solidity} can be due to the network failing to learn the representation of such high \textit{solidity} values, or the presence of many artefacts in the corresponding generated levels leading to a incorrect calculation of the feature. 
 It is worth noting that due to the sampling issues we considered in the previous chapters, this experiment has been conducted by selecting different feature vectors that exhibited the desired value on a particular feature. The possible drawback of this experiment is that the visualized distributions could depend on other factors other than the single requested feature value, however we observed better or similar results in earlier network we trained with different settings, suggesting that the issue we just highlighted could have marginal impact.

 
\subsection{Final Considerations}
\paragraph{} Results we obtained indicate that the conditional network has some advantages over the unconditioned version, which can be briefly resumed as a better overall sample quality (Sample Evaluation Metrics), a better learning of input and other topological features (experiments 1 and 2) and the possibility to control the level generation up to some extent (Experiment 3). In a last consideration about our results we want to highlight that our experiments focused on proving the ability of the networks to reproduce the true feature distributions of the original dataset. This means that if the null hypothesis for one feature cannot be rejected then we can assume that its distribution may be "close enough" to the true one to generate levels which are similar to the existing ones. The contrary, however, does not necessarily prevents the model to be effectively used as a tool for generating levels, but only indicates that the generated levels somehow differs from the true ones. This fact can be appreciated by a visual comparison of the true and generated samples we proposed in section~\ref{sec:samples}: in many cases it is possible to notice that the global shape and size of the generated samples vaguely resembles that of the true levels, indicating that the network actually consider some of the requested features up to a certain point. On the same figure it is possible to notice some struggle of the network in representing smaller features such level borders, probably due to generation noise. This can alter the calculated features and lead to inaccurate distributions, for this reason we recommend to apply some morphological processing or noise reduction techniques before converting the generated images to playable WAD files in order to reduce the generation noise.

\paragraph{} When using this type of models, one of the most common concerns is that the network could overfit the training set, learning to reproduce samples from the dataset. While we cannot prove it formally, we refer to the results of \citeauthor{empiricalevaluation} in \cite[Appendix~C]{empiricalevaluation}, which claim that overfit is difficult to occur in the type of model we used, even for a small number of training samples, de facto demonstrating that the behaviour of GANs is different from that of classical deep neural networks used for classification.

\section{Conclusions}
\label{sec:conclusions}
\paragraph{} Although the model we designed is far for being perfect in solving the level generation problem for 2d non-linear environment such DOOM maps, we think it's still a good starting point for future improvements and could represent a viable alternative to classical Procedural Generation. In particular, most of the levels generated from the network have proved to be interesting to explore and play due to the presence of particular features typical of doom maps, such as narrow tunnels and large rooms. This suggest that one of the advantages of our method with respect to Procedural Generation is that in our case there is no need of an expert designer to embed it's knowledge in the generation process; still, this method allows the designer to focus on the selection of more high-level features as those we selected as network inputs. This generative method, which is commonly used to produce visually appealing images, proved to be applicable to a topological setting like ours, even if not without issues: While in creating a picture of a face, a small variation in colour intensity is quite unnoticeable and can be tolerated, in our domain even a pixel-sized difference in a level map could alter drastically the level topology itself, for example creating a new access between two areas. This issue is emphasized by the output noise which is quite common in samples generated by a GAN.



\section{Future Work}
\label{sec:futurework}
\paragraph{} In this section we first present the general open problems we encountered in our setting, then we propose some specific works that can be made to improve our system.
\subsection{Open Problems}
\subsubsection{Data Availability}
\paragraph{} The amount of data available is a problem that affects in general every deep learning setting. As discussed in earlier sections, the context of video-games is one of field in which data is less available and uniform. In our work we used a dataset of 1088 levels which have been augmented by rotation due to memory size constraints, but we envision that a larger amount of levels could make the network more accurate in generating levels. We propose a possible improvement for our system in section \ref{sec:data-augmentation}.
\subsubsection{Samples Evaluation}
\paragraph{} As we explained in section~\ref{sec:evaluation}, the problem of evaluating the samples generated by a GAN is still a recent field of research and a general prevailing model still have to be proposed. Moreover, the particular domain of our work makes even more difficult to apply the commonly used methods to assess quality. In section~\ref{sec:evaluation} we proposed a qualitative method for assessing the generated sample quality during the training process according to our data. While the method we applied succeeds in indicating that the network is actually learning the level structures, the metrics we proposed have the drawback that they need to be calculated on each map differently in order to benefit of their informational power, while considered altogether for assessing the general sample quality. 

\subsubsection{Loss of accuracy}
\paragraph{} The system we designed assumes that one pixel is equivalent to 32 Doom Map Units since it's the diameter of the smallest object in DOOM. While this ensures that objects cannot overlap on the image representation, it introduces an unavoidable loss of accuracy in object positioning. Moreover, using a single pixel for representing objects such as in "Thing Maps" makes the task of distinguish generated object from noise and artefacts more difficult. While we weren't able to detect checker board artefacts in the structural maps such as the floormap, wallmap and heightmaps, the generated \glspl{thingsmap} often shows a placement of objects that is too regular, resembling the issue discussed in section~\ref{sec:artefacts-reduction}. We envision that an increase of dataset resolution, for example setting one pixel size to 16 MU instead of 32, would help in reducing this problem, at the cost of choosing between an high resolution and the inclusion of more levels from the dataset.

\subsubsection{Improving Network Sampling}
\label{sec:sampling}
\paragraph{} Due to the high dimensionality of the feature space, sampling the resulting network using arbitrary feature values often results in low quality samples, for this reason we sample the network around points in the feature space for which the network have been trained. However, in real application it could be better to have the designer specify his own desired input features, such the desired area, the balance between large halls and corridors, etc. We envision that this issue could be reduced using more training data and possibly using soft labels as proposed by \cite{improved_gan}, at the cost of an increase of noise in the output. An alternative design could be the implementation of the "Content Sampling" introduced in section \ref{sec:usecase_sampling}.

\subsection{Possible Applications and future develops}
\subsubsection{Augmenting input data}
\label{sec:data-augmentation}
\paragraph{} In our work we only used levels having a single "floor", or connected figure, in order to prevent the network for learning unnecessary patterns like the position or rotation of the various floors that compose the level when they actually are irrelevant from a gameplay point of view. One method for increasing the number of training samples is re-generating the dataset considering each floor as a separated level and calculating the features on a floor basis. An additional method for using a larger part of the data we collected is to simply increase the input/output sample size, although this require more computational capabilities. 

\subsubsection{Improving samples to WAD conversion}
\paragraph{} In our work we focused on the generation process, while we also provided a simple method for playing the generated levels in DOOM. However, this method is still not perfect: only preliminary work have been done on applying the height differences in levels, and no visual improvements such as automatic texture selection have been implemented yet. Moreover, work have still to be done for decorating the level with doors, elevators and switches. 

 \subsubsection{Sample interpolation and Style Transfer}
\paragraph{} An interesting practice for generating diverse samples and assessing the generalization capabilities of the network is the interpolation of samples in the feature space \cite{slerp}. We provided a method for sampling interpolated batches of levels by providing a start and end point in feature space, while keeping the noise vector fixed. If we consider adding levels from other games to the dataset by converting them to the same domain we used, this technique can be easily used for interpolating levels from different game and study what different features they exhibit.

\subsubsection{Tweaking the model}
% Alternative models
\paragraph{} For increasing the model capabilities a lot of work can still be done. One first attempt could be changing the network hyperparameters in order to find a setting that allows the network to learn a better representation. This work has been attempted while searching a good architecture and we didn't find any improvements over the default settings, although we cannot prove we are using optimal values.

\subsubsection{Different architectures}
% Hybrid method based on autoencoders
\paragraph{} Due to the constant research in generative models, there is an always increasing number of different new models to try and experiment in order to improve the generation of samples. In our work we limited our search only to the mostly used pure-GAN models which could easily run on our machines, but a large number of models are available.
For example, one method that showed good results on faces generation that mixes GANs and Autoencoders is the BEGAN model proposed by \citeauthor{gan:BEGAN} in \cite{gan:BEGAN}. Another approach could be the one for high-resolution images such as in \textit{\citetitle{gan:high-res}} \cite{gan:high-res}, showing interesting improvements in visual quality by growing the generator and the discriminator progressively.

\subsubsection{Possible Applications}
The most obvious application of our work would be the case of off-line video-game level generation. In particular our work finds application whenever it is needed to obtain 3d maps which doesn't overlap on the height axis. Since the only requirement for the levels to work with our framework is the one we just stated, it is possible to extend this system to other environments and domains by just changing the feature extraction modules. In other words, the system is independent of the technology of the particular game we used for training, and it can be used virtually in any type of 3d game or simulation in which the user can move in two dimensions, eventually with changes of floor height. Possible applications in another fields could be the generation of environments for training or testing the behaviour of AI agents, and the use of the discriminator network for classification tasks by adding a last layer and fine tuning the network. However, due to the set-up requirements currently needed for running the project, a most probable application that we envision of our work is as a starting point for the development of more complex systems or more advanced studies on Procedural Content Generation via Machine Learning applied to this type of environments.

