\chapter{Experiment Design}
\label{ch:experiment}

% TODO: Scrivi intro
\paragraph{}


\section{Experiment setup}
\label{sec:experiments}
\paragraph{} In this section we describe the preliminary steps for conducting the experiments with the system described in the previous chapter. The next section will describe the experiments in detail.

\subsection{Input Levels Selection}
\label{sec:InputSelection}
\paragraph{} For preparing our experiments we filtered the DoomDataset by taking only the samples up to 128x128 in size and which had exactly 1 "floor". This led to a dataset of about 1000 samples, which are then augmented by rotation during the training process. This is motivated from the fact that even if the level orientation does not affect playability, using levels with more floors could lead the network to learn a correlation between floors (and how to arrange them inside the map) which could potentially be misleading or be just enforced by the sample size or the way the editor arranged them on the level coordinate space. Moreover, using only one-floor levels helped in reducing smaller artefacts that appeared as very small floors in resulting output.



\subsection{Framework Evaluation}
\label{sec:modelevaluation}
\paragraph{} In order to evaluate the feasibility of our approach on the problem of level generation, we designed a set of experiments to test some of the settings that are possible to use with our framework. 
All the experiments involve comparing the distribution of true data and the one generated from the neural network, according to the features that is possible to calculate from the images generated from different models. In generating the models, we first trained a neural network without input features so that the generator is only controlled by the noise vector $Z$. We then added a set of features to our architecture and used it to train two networks for a different number of iterations. The third architecture we designed is a deeper network with less features and feature maps. Details on the network we produced are shown in table~\ref{trained_models}.


\subsection{Feature Selection}
In selecting which numerical features $Y$ to use as network input we followed some assumption and criteria:
\begin{enumerate}
	\item \textit{Reconstructability:} In order to be able to analyse the resulting network, it is possible to use only features that can be reconstructed from the network output images with reasonable loss of information. For example, features what depends on the WAD representation of the level such as the number of sectors are too much dependant both on the editor used to build the level, and the algorithm we use to convert back images to WAD. On the contrary, features based on the Feature Maps are a better choice since they are evaluated using the same algorithm.
	\item \textit{Visibility:} In order to be selected as an input, a feature has to have a visual impact on the samples. While in principle neural networks can extract complex and possibly inscrutable structure in the data, we decided to use only features that can have a visual feedback on the Feature Maps.
	\item \textit{Robustness:} Other than having a visual impact on the samples, a good feature must be robust to noise in the feature maps, in other words it must not change greatly for small variation in the pixel colour the images. 
\end{enumerate}


\paragraph{} We selected a subset of features that reasonably satisfied these properties by comparing the feature values of a small number of visually different level sets. An example of level set and the corresponding values for the selected features is shown on figure~\ref{fig:feature_selection}. A summary of the feature we used in our experiment is:
\begin{itemize}
	\item \textit{level equivalent diameter}: Diameter of the circle having the same area as the level.
	\item \textit{level major axis length}: Length of the longest axis of the level, independently from the rotation.
	\item \textit{level minor axis length}: Length of the shortest axis of the level, independently from the rotation.
	\item \textit{level solidity}: Ratio between the area of the level and its convex hull. It's a metric of the level convexity.
	\item \textit{nodes (number of rooms)}: Count of the rooms in the room adjacency graph.
	\item \textit{distmap skewness}: Skewness of the distribution of each pixel distance from the closest wall. This metric is visually related to the balance between large and small areas.
	\item \textit{distmap kurtosis}: Kurtosis of the distribution of each pixel distance from the closest wall. This metric is visually related to the presence of both very large and very small areas, or area size variety.
\end{itemize}


 \begin{figure}[h!]
 	\includegraphics[width=\linewidth]{feature_selection.png}
 	\caption[Example of feature values]{Example of feature values on a set of 5 different levels. The first row shows the Room Map of the levels, in which each room is enumerated with a different grayscale colour. The second row shows the feature values for the features \textit{level equivalent diameter}, \textit{level major axis length}, \textit{level minor axis length}, \textit{level solidity}, \textit{nodes (number of rooms)}, \textit{distmap skewness} and \textit{distmap kurtosis}. }
 	\label{fig:feature_selection}
 \end{figure}

\newpage
\subsection{Trained Architectures}
\paragraph{} In training the models we kept fixed the WGAN-GP architecture, learning hyperparameters, the kernel size and the stride, while we varied the input features and the number of layers. 

\begin{table}[h!]
	\begin{tabularx}{\textwidth}{| c | c | X | X | X | X |}
		\hline
		\textbf{Run Name} & \textbf{Iterations} & \textbf{Features} & \textbf{Maps} & \textbf{D Layers (filters)} & \textbf{G Layers (filters)} \\
		\hline
		26k & 26000 & 
		\begin{itemize}
			\raggedright
			\small
			\item[] level equivalent diameter
			\item[] level major axis length
			\item[] level minor axis length
			\item[] level solidity
			\item[] nodes
			\item[] distmap-skew
			\item[] distmap-kurt
		\end{itemize}
		 & 
		 	\begin{itemize}
		 	\raggedright
		 	\small
		 	\item[] floormap
		 	\item[] heightmap
		 	\item[] wallmap
		 	\item[] thingsmap
		 \end{itemize}
		 & 4 (1024, 512, 256, 128) & 4 (128, 256, 512, 1024)\\
		
		\hline
		
		12k & 12000 & 
		\begin{itemize}
			\raggedright
			\small
			\item[] level equivalent diameter
			\item[] level major axis length
			\item[] level minor axis length
			\item[] level solidity
			\item[] nodes
			\item[] distmap-skew
			\item[] distmap-kurt
		\end{itemize}
		& 
		\begin{itemize}
			\raggedright
			\small
			\item[] floormap
			\item[] heightmap
			\item[] wallmap
			\item[] thingsmap
		\end{itemize}
		& 4 (1024, 512, 256, 128) & 4 (128, 256, 512, 1024)\\
		
		\hline
		
		unconditional & 15000 & 
		No features
		& 
		\begin{itemize}
			\raggedright
			\small
			\item[] floormap
			\item[] heightmap
			\item[] wallmap
			\item[] thingsmap
		\end{itemize}
		& 4 (1024, 512, 256, 128) & 4 (128, 256, 512, 1024)\\
		
		\hline
		
		deeper & 11000 & 
		\begin{itemize}
			\raggedright
			\small
			\item[] level equivalent diameter
			\item[] level major axis length
			\item[] level minor axis length
			\item[] level solidity
			\item[] nodes
		\end{itemize}
		& 
		\begin{itemize}
			\raggedright
			\small
			\item[] floormap
			\item[] wallmap
			\item[] thingsmap
		\end{itemize}
		& 8 (1024, 1024, 512, 512, 256,  256, 128, 128) & 8 (128, 128, 256, 256, 512, 512, 1024, 1024)\\
		\hline
		
	\end{tabularx}
	\caption[ Trained Models ]{ Trained network. }
	\label{tab:trained_models}
\end{table}	

\subsection{Experiments description}
\subsubsection{Experiment 1}
In the first experiment we tested the ability of the "unconditional" network to generate levels that exhibit similar features to the original levels. 
For each level in the dataset we sampled a level from the generator network using random noise, we extracted the features from the generated level and we compared the distribution of the dataset with the distribution of the generated features. %TODO: Scrivi dove andare a leggere i risultati

\subsubsection{Experiment 2}
In the second experiment we tested if the addition of features to a network can have some effect on the generation of the samples. For doing this, we trained a model 
\subsubsection{Experiment 3}
In the third experiment we tested if the input features in networks "12k" and "26k" can be used to control the generation of the levels, by showing the distribution of multiple samples with respect to a set of feature values in input. Since an exhaustive sampling on every true level would have been computationally expensive, we only selected for each input feature the three levels corresponding to the 25th, 50th and 75th percentile of the dataset distribution for that feature. %TODO: Genera e mostra i livelli che hai selezionato?

%TODO: Scrivi summary e vedi se tenere i results come una sezione o un capitolo
